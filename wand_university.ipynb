{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5bdaa3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pydantic in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.9.2)\n",
      "Requirement already satisfied: openai in ./micromamba/envs/vllm/lib/python3.11/site-packages (1.58.1)\n",
      "Requirement already satisfied: datasets in ./micromamba/envs/vllm/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: trl in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.11.4)\n",
      "Requirement already satisfied: transformers in ./micromamba/envs/vllm/lib/python3.11/site-packages (4.47.1)\n",
      "Requirement already satisfied: peft in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: torch in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: wandb in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.18.5)\n",
      "Requirement already satisfied: vllm in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.6.5)\n",
      "Requirement already satisfied: tqdm in ./micromamba/envs/vllm/lib/python3.11/site-packages (4.66.5)\n",
      "Requirement already satisfied: fundus in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.4.4)\n",
      "Requirement already satisfied: arxiv in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: clize in ./micromamba/envs/vllm/lib/python3.11/site-packages (5.0.2)\n",
      "Requirement already satisfied: xformers in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.0.28.post3)\n",
      "Requirement already satisfied: flash-attn in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.7.2.post1)\n",
      "Requirement already satisfied: llama-index in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.12.8)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pydantic) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: filelock in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: accelerate in ./micromamba/envs/vllm/lib/python3.11/site-packages (from trl) (1.2.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from trl) (0.8.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: psutil in ./micromamba/envs/vllm/lib/python3.11/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: networkx in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (5.28.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: sentencepiece in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: blake3 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (1.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.115.2)\n",
      "Requirement already satisfied: uvicorn[standard] in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.32.0)\n",
      "Requirement already satisfied: pillow in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (10.4.0)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.21.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (7.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.7.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.10.9)\n",
      "Requirement already satisfied: outlines==0.1.11 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.1.11)\n",
      "Requirement already satisfied: xgrammar>=0.1.6 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.1.7)\n",
      "Requirement already satisfied: partial-json-parser in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.2.1.1.post4)\n",
      "Requirement already satisfied: pyzmq in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (26.2.0)\n",
      "Requirement already satisfied: msgspec in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.18.6)\n",
      "Requirement already satisfied: gguf==0.10.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (8.5.0)\n",
      "Requirement already satisfied: mistral_common>=1.5.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from mistral_common[opencv]>=1.5.0->vllm) (1.5.0)\n",
      "Requirement already satisfied: einops in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.8.0)\n",
      "Requirement already satisfied: compressed-tensors==0.8.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: depyf==0.18.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.18.0)\n",
      "Requirement already satisfied: ray>=2.9 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (2.37.0)\n",
      "Requirement already satisfied: nvidia-ml-py>=12.560.30 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (12.560.30)\n",
      "Requirement already satisfied: torchvision==0.20.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.20.1)\n",
      "Requirement already satisfied: astor in ./micromamba/envs/vllm/lib/python3.11/site-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: interegular in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: lark in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (1.2.2)\n",
      "Requirement already satisfied: nest_asyncio in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (3.1.0)\n",
      "Requirement already satisfied: diskcache in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
      "Requirement already satisfied: referencing in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
      "Requirement already satisfied: pycountry in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (20241001)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
      "Requirement already satisfied: lxml<6,>=4.9 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (5.3.0)\n",
      "Requirement already satisfied: more-itertools<10,>=9.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (9.1.0)\n",
      "Requirement already satisfied: cssselect<2,>=1.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (1.2.0)\n",
      "Requirement already satisfied: feedparser<7,>=6.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (6.0.11)\n",
      "Requirement already satisfied: colorama<1,>=0.4 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (0.4.6)\n",
      "Requirement already satisfied: langdetect<2,>=1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (1.0.9)\n",
      "Requirement already satisfied: validators<1,>=0.24 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (0.34.0)\n",
      "Requirement already satisfied: fastwarc<1,>=0.14 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (0.14.9)\n",
      "Requirement already satisfied: chardet<6,>=5.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (5.2.0)\n",
      "Requirement already satisfied: dict2xml<2,>=1.7.6 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (1.7.6)\n",
      "Requirement already satisfied: xmltodict<1,>=0.13.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (0.14.1)\n",
      "Requirement already satisfied: sigtools>=4.0.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from clize) (4.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from clize) (24.2.0)\n",
      "Requirement already satisfied: od in ./micromamba/envs/vllm/lib/python3.11/site-packages (from clize) (2.0.2)\n",
      "Requirement already satisfied: docutils>=0.17.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from clize) (0.21.2)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.8 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.12.8)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.6.3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.3.12)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: six>=1.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.40.0)\n",
      "Requirement already satisfied: brotli in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fastwarc<1,>=0.14->fundus) (1.1.0)\n",
      "Requirement already satisfied: sgmllib3k in ./micromamba/envs/vllm/lib/python3.11/site-packages (from feedparser<7,>=6.0->fundus) (1.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (1.15.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: certifi in ./micromamba/envs/vllm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./micromamba/envs/vllm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index) (2.0.36)\n",
      "Requirement already satisfied: dataclasses-json in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.2.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (9.0.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.17.0)\n",
      "Requirement already satisfied: llama-cloud>=0.1.5 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.6)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.5.18)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from mistral_common[opencv]>=1.5.0->vllm) (4.10.0.84)\n",
      "Requirement already satisfied: joblib in ./micromamba/envs/vllm/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from ray>=2.9->vllm) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (13.9.2)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: pybind11 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from xgrammar>=0.1.6->vllm) (2.13.6)\n",
      "Requirement already satisfied: pytest in ./micromamba/envs/vllm/lib/python3.11/site-packages (from xgrammar>=0.1.6->vllm) (8.3.4)\n",
      "Requirement already satisfied: zipp>=3.20 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from importlib_metadata->vllm) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (0.6.2)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.20.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.8->llama-index) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.8->llama-index) (3.23.2)\n",
      "Requirement already satisfied: iniconfig in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pytest->xgrammar>=0.1.6->vllm) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pytest->xgrammar>=0.1.6->vllm) (1.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pydantic openai datasets trl transformers peft torch pandas wandb vllm tqdm fundus arxiv datasets clize xformers flash-attn llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "61568d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M3P: Learning Universal Representations via Multitask Multilingual\n",
      "Multimodal Pre-training\n",
      "Minheng Ni1*† Haoyang Huang2† Lin Su3† Edward Cui3 Taroon Bharti3 Lijuan Wang4\n",
      "Jianfeng Gao5 Dongdong Zhang2 Nan Duan2‡\n",
      "1 Research Center for Social Computing and Information Retrieval\n",
      "Harbin Institute of Technology, China\n",
      "2 Natural Language Computing, Microsoft Research Asia, China\n",
      "3 Bing Multimedia Team, Microsoft, China\n",
      "4 Cloud+AI, Microsoft, United States\n",
      "5 Deep Learning, Microsoft Research Redmond, United States\n",
      "mhni@ir.hit.edu.cn\n",
      "{haohua, lins, edwac, tbharti, lijuanw, jfgao, Dongdong.Zhang, nanduan}@microsoft.com\n",
      "Abstract\n",
      "We present M3P, a Multitask Multilingual Multimodal\n",
      "Pre-trained model that combines multilingual pre-training\n",
      "and multimodal pre-training into a uniﬁed framework via\n",
      "multitask pre-training. Our goal is to learn universal repre-\n",
      "sentations that can map objects occurred in different modal-\n",
      "ities or texts expressed in different languages into a com-\n",
      "mon semantic space. In addition, to explicitly encourage\n",
      "ﬁne-grained alignment between images and non-English lan-\n",
      "guages, we also propose Multimodal Code-switched Train-\n",
      "ing (MCT) to combine monolingual pre-training and multi-\n",
      "modal pre-training via a code-switch strategy. Experiments\n",
      "are performed on the multilingual image retrieval task across\n",
      "two benchmark datasets, including MSCOCO and Multi30K.\n",
      "M3P can achieve comparable results for English and new\n",
      "state-of-the-art results for non-English languages.\n",
      "1. Introduction\n",
      "Recently, we witness the rise of a new paradigm of natu-\n",
      "ral language processing (NLP), where general knowledge is\n",
      "learned from raw texts by self-supervised pre-training and\n",
      "then applied to downstream tasks by task-speciﬁc ﬁne-tuning.\n",
      "Now, these state-of-the-art monolingual pre-trained language\n",
      "models, such as BERT [7], RoBERTa [23] and GPT-2 [28],\n",
      "have been expanded to multilingual scenarios, such as Mul-\n",
      "tilingual BERT [ 7], XLM/XLM-R [ 5, 4], Unicoder [ 13].\n",
      "*Work is done during an internship at Microsoft Research Asia.\n",
      "†These authors contributed equally to this work.\n",
      "‡Corresponding Author.\n",
      "Moreover, some pre-training models under multimodal sce-\n",
      "narios, such as Unicoder-VL [19], UNITER [3], ERNIE-ViL\n",
      "[36], VILLA [10] and Oscar [21], also come out.\n",
      "However, it is still challenging to extend these pre-trained\n",
      "models to multilingual-multimodal scenarios. The multilin-\n",
      "gual pre-trained language models cannot handle vision data\n",
      "(e.g., images or videos) directly, whereas many pre-trained\n",
      "multimodal models are trained on English corpora thus can-\n",
      "not perform very well on non-English languages. Therefore,\n",
      "high quality multilingual multimodal training corpus is es-\n",
      "sential to combine multilingual pre-training and multimodal\n",
      "pre-training. However, there are only a few multilingual\n",
      "multimodal corpora exist, and they also have low language\n",
      "coverage. Moreover, relying on high-quality machine transla-\n",
      "tion engines to generate such data from English multimodal\n",
      "corpora is both time-consuming and computationally ex-\n",
      "pensive. Learning explicit alignments between vision and\n",
      "non-English languages during pre-training is lacking.\n",
      "To address these challenges, this paper presents M3P,\n",
      "a Multitask Multilingual Multimodal Pre-trained model,\n",
      "which aims to learn universal representations that can map\n",
      "objects occurred in different modalities or texts expressed in\n",
      "different languages into a common semantic space. In order\n",
      "to alleviate the issue of lacking enough non-English labeled\n",
      "data for multimodal pre-training, we introduce Multimodal\n",
      "Code-switched Training (MCT) to enforce the explicit align-\n",
      "ments between images and non-English languages. The goal\n",
      "is achieved by (i) learning to represent multilingual data us-\n",
      "ing multilingual corpora (e.g., sentences from Wikipedia cov-\n",
      "ering 100 languages) by multilingual pre-training, (ii) learn-\n",
      "ing multilingual-multimodal representations by randomly\n",
      "replacing some English words with their translations in other\n",
      "1\n",
      "arXiv:2006.02635v4  [cs.CL]  1 Apr 2021\n",
      "languages from multimodal corpora (e.g., image-caption\n",
      "pairs labeled in English), and (iii) generalizing these rep-\n",
      "resentations to deal with multilingual-multimodal tasks by\n",
      "Multitask learning.\n",
      "In summary, the main contributions of this paper are:\n",
      "• We present M3P, the ﬁrst known effort on combining\n",
      "multilingual pre-training and multimodal pre-training\n",
      "into a uniﬁed framework.\n",
      "• We propose a novel Multimodal Code-switched Train-\n",
      "ing (MCT) method, an effective way to enhance the\n",
      "multilingual transfer ability of M 3P in the zero-shot\n",
      "and few-shot settings.\n",
      "• We achieve new state-of-the-art results for the multi-\n",
      "lingual image-text retrieval task on both Multi30K and\n",
      "MSCOCO for non-English languages, outperforming\n",
      "existing multilingual methods by a large margin. The\n",
      "proposed model can also achieve comparable results\n",
      "for English on these two datasets, compared to the state-\n",
      "of-the-art monolingual multimodal models.\n",
      "• Last but not least, we conduct extensive experiments\n",
      "and analysis to provide insights on the effectiveness of\n",
      "using Multimodal Code-switched Training (MCT) and\n",
      "each pre-training task.\n",
      "2. Related Work\n",
      "Multilingual Pre-trained Models Multilingual BERT\n",
      "(M-BERT) [7] demonstrates that by performing masked lan-\n",
      "guage modeling on a multilingual corpus with shared vo-\n",
      "cabulary and weights for 102 languages, surprisingly good\n",
      "results can be achieved on the cross-lingual natural language\n",
      "inference (XNLI) [6] task in 15 languages. XLM [ 5] and\n",
      "Unicoder [13] further improve the multilingual BERT by\n",
      "introducing new pre-training tasks based on a bilingual cor-\n",
      "pus. However, all such models work for NLP tasks and are\n",
      "not well designed for multimodal tasks such as Multilingual\n",
      "Image-text Retrieval or Multimodal Machine Translation.\n",
      "Multimodal Pre-trained Models Recently, a large num-\n",
      "ber of multimodal pre-trained models, such as ViLBERT\n",
      "[24], Unicoder-VL [19], UNITER [3], VLP [37] and Oscar\n",
      "[21], are developed for vision-language tasks using multi-\n",
      "layer Transformer as the backbone. However, as it is not\n",
      "easy to collect well-aligned visual-linguistic training data in\n",
      "multiple languages, all these models are pre-trained for En-\n",
      "glish only based on monolingual multimodal corpora, such\n",
      "as Conceptual Captions [ 29], SBU Captions [ 26], Visual\n",
      "Genome [17] and MSCOCO [2]. Hence, it is not feasible to\n",
      "apply them into multimodal tasks with non-English inputs.\n",
      "Code-switched Training Code-switched training [ 27]\n",
      "[33] converts the original training corpus to code-switched\n",
      "corpus, which can help the model explicitly model the rela-\n",
      "tionship among corresponding words in different languages.\n",
      "Existing work uses a rule-based word replacement strategy\n",
      "to replace the original word with translated word randomly\n",
      "by bilingual dictionaries. This approach provides a signiﬁ-\n",
      "cant improvement to the low-resource language. However,\n",
      "existing works use Code-switching for text-only tasks and ig-\n",
      "nore its application on multimodal pre-training model under\n",
      "multilingual-multimodal scenarios.\n",
      "3. M3P: Multitask Multilingual Multimodal\n",
      "Pre-training\n",
      "In this section, we describe how we train M 3P using a\n",
      "multilingual-monomodal corpus (e.g., sentences extracted\n",
      "from Wikipedia) and a monolingual-multimodal corpus (e.g.,\n",
      "English image-caption pairs). As outlined in Figure 1, we\n",
      "use the self-attentive transformer architecture of BERT, and\n",
      "design two pre-training objectives with three types of data\n",
      "streams. Multitask training is employed into the pre-training\n",
      "stage to optimize all pre-training objectives simultaneously\n",
      "for better performance. We optimize the accumulated loss\n",
      "of both pre-training objectives with the same weight in each\n",
      "iteration to train them by turns.\n",
      "3.1. Data Stream\n",
      "We use two basic data streams, Multilingual Monomodal\n",
      "Stream and Monolingual Multimodal Stream, from the multi-\n",
      "lingual corpus and multimodal corpus, respectively. We also\n",
      "design Multimodal Code-switched Stream to utilize multi-\n",
      "lingual data and multimodal data at the same time. Details\n",
      "regarding the three data streams are introduced below.\n",
      "Multilingual Monomodal Stream To apply multilingual\n",
      "pre-training, we use Multilingual Monomodal Stream as\n",
      "model input. Given an input text in any language w[li], we\n",
      "ﬁrst tokenize it into a sequence of BPE tokens via Sentence\n",
      "Piece [18]. Then we can obtain a text representation se-\n",
      "quence by summing up the text embedding and the position\n",
      "embedding of each BPE token. Moreover, a language em-\n",
      "bedding [5] is added to each token to indicate its language\n",
      "attribute. Speciﬁcally, the input data is deﬁned as:\n",
      "{\n",
      "w[li]\n",
      "}\n",
      "=\n",
      "{\n",
      "(w[li]\n",
      "1 ,w[li]\n",
      "2 ,..., w[li]\n",
      "M )\n",
      "}\n",
      "where M denotes the length of w[li] and li denotes a lan-\n",
      "guage in the language set L. We denote this stream as D[X].\n",
      "Monolingual Multimodal Stream To apply multimodal\n",
      "pre-training, we use Monolingual Multimodal Stream as\n",
      "model input. Given a pair of English text and image\n",
      "2\n",
      "Figure 1: Three data streams and four pre-training tasks used in M3P. Blue blocks denote English text, and Yellow, Green and\n",
      "Orange blocks denote non-English text.\n",
      "(w[EN],v), the text representation sequence of w[EN] is ob-\n",
      "tained similarly as we described in Multilingual Monomodal\n",
      "Stream section, where English is used as the language em-\n",
      "bedding. For the image v, we use Faster-RCNN [ 12] to\n",
      "detect image regions and use corresponding visual features\n",
      "in each region as a visual feature sequence. We also add a\n",
      "spatial embedding to each visual token, which is a 5-D vector\n",
      "based on its normalized top-left, bottom-right coordinates,\n",
      "and the fraction of the image area covered. We project these\n",
      "two vectors to the same dimension of the text representation\n",
      "using two fully-connected (FC) layers. Therefore, the im-\n",
      "age representation sequence is obtained by summing up its\n",
      "projected visual feature vector and spatial embedding vector\n",
      "of each region in the image. Furthermore, we add a stream\n",
      "tag [IMG] at the beginning of the image region sequence to\n",
      "separate text tokens and image tokens, and concatenate them\n",
      "to form an input stream:\n",
      "{\n",
      "w[EN],v\n",
      "}\n",
      "=\n",
      "{\n",
      "(w[EN]\n",
      "1 ,w[EN]\n",
      "2 ,..., w[EN]\n",
      "M ),(v1,v2,..., vN)\n",
      "}\n",
      "We denote this stream as D[EN].\n",
      "Multimodal Code-switched Stream We generate Multi-\n",
      "modal Code-switched Stream from Monolingual Multimodal\n",
      "Stream by code-switched method, given English text and\n",
      "image pairs (w[EN],v), the set of code-switched languages\n",
      "C = {c1,c2,...,c k}, and bilingual dictionaries which can\n",
      "translate a word from English to any language ci. Follow-\n",
      "ing [27], for each word w[EN]\n",
      "i in English text w[EN], we\n",
      "replace it with a translated word with a probability of β. If\n",
      "a word has multiple translations, we choose a random one.\n",
      "Similar to the generation process of Multilingual Monolin-\n",
      "gual Stream, we obtain the text representation sequence of\n",
      "the Code-switched text w[C] in the same way while keep-\n",
      "ing the original language embedding.* Similar with Mono-\n",
      "lingual Multimodal Stream, the text and image representa-\n",
      "tion sequences are concatenated as the ﬁnal input stream:{\n",
      "(w[d1]\n",
      "1 ,w[d2]\n",
      "2 ,..., w[dM]\n",
      "M ),(v1,v2,..., vN)\n",
      "}\n",
      ", where di is a\n",
      "random language in {EN}∪C. We simplify the input se-\n",
      "quence as:\n",
      "{\n",
      "w[C],v\n",
      "}\n",
      "=\n",
      "{\n",
      "(w[C]\n",
      "1 ,w[C]\n",
      "2 ,..., w[C]\n",
      "M ),(v1,v2,..., vN)\n",
      "}\n",
      "We denote this stream as D[C].\n",
      "3.2. Pre-training Objectives\n",
      "To pre-train M3P under multilingual-multimodal scenario,\n",
      "we designed two types of pre-training objectives. Multilin-\n",
      "gual Training aims to learn grammar or syntax from well-\n",
      "formed multilingual sentences. Multimodal Code-switched\n",
      "Training (MCT) aims to learn different languages from the\n",
      "shared vision modal and the alignment between vision and\n",
      "non-English texts.\n",
      "3.2.1 Multilingual Training\n",
      "Multilingual Masked Language Modeling (xMLM)\n",
      "Similar to Multilingual BERT [7], XLM [5] and Unicoder\n",
      "[13], this task performs masked language modeling based\n",
      "on the multilingual corpus. At each iteration, a batch is\n",
      "composed of sentences sampled from different languages.\n",
      "The sampling probability of a language li is deﬁned as\n",
      "λli = pα\n",
      "li/∑\n",
      "li pα\n",
      "li, where pli is the percentage of li in the\n",
      "entire multilingual corpus, and the smoothing factor αis set\n",
      "to 0.3. In each batch, we randomly sample 15% of the words\n",
      "and (i) replace them with a special symbol [MASK], (ii) re-\n",
      "place them with random tokens, or (iii) keep them unchanged\n",
      "*We have tried to change language embedding in Code-switched Stream,\n",
      "but no signiﬁcant gain was obtained.\n",
      "3\n",
      "with a probability of 80%, 10% and 10%, respectively. We\n",
      "only use Multilingual Monomodal Stream D[X] for we do\n",
      "not need to use Code-switching to extend it to multilingual\n",
      "corpus. The loss function is deﬁned as:\n",
      "LxMLM(θ) =−Ew[li]∼D[X] log qθ(w[li]\n",
      "m |w[li]\n",
      "\\m)\n",
      ", where w[li]\n",
      "m is the masked token and w[li]\n",
      "\\m is its context.\n",
      "3.2.2 Multimodal Code-switched Training\n",
      "Because of the lack of labeled data for the non-English mul-\n",
      "timodal scenario, the model can only learn multilingualism\n",
      "and multimodality independently. To help the model learn\n",
      "different language representations under the shared vision\n",
      "modal, we propose three Multimodal Code-switched Train-\n",
      "ing tasks: MC-MLM, MC-MRM and MC-VLM. We mix\n",
      "Multimodal Code-switched Stream D[C] and Monolingual\n",
      "Multimodal Stream D[EN] with a proportion ratio of αand\n",
      "1 −α, respectively, in train these tasks. To simplify the\n",
      "symbols, we denote the mixed data stream as Dand omit\n",
      "the mask [EN] or [C] as [·].\n",
      "Multimodal Code-switched Masked Language Model-\n",
      "ing (MC-MLM) Different from the pre-training tasks in\n",
      "ViLBERT [24] and Unicoder-VL [19], this task aims to learn\n",
      "the representation of different languages based on the shared\n",
      "vision modal. Mixed data stream Dis used for training this\n",
      "objective. Speciﬁcally, the model predicts each masked to-\n",
      "ken w[·]\n",
      "m in the caption w[·] based on its surrounding tokens\n",
      "w[·]\n",
      "\\m and all image regions v. We follow the same masking\n",
      "strategy used in xMLM to mask tokens in the input caption.\n",
      "The loss function is deﬁned as:\n",
      "LMC−MLM(θ) =−E(w[·],v)∼Dlog pθ(w[·]\n",
      "m|w[·]\n",
      "\\m,v)\n",
      ", where Dis the mixed data stream.\n",
      "Multimodal Code-switched Masked Region Modeling\n",
      "(MC-MRM) This task aims to learn vision representations\n",
      "with multilingual text as the context in mixed data stream\n",
      "D. The model reconstructs each masked image region vn\n",
      "based on the remaining regions v\\n and all caption tokens\n",
      "w[·]. We randomly mask image regions with a probability of\n",
      "15%. The input representation of each masked image region\n",
      "is set to zeros or kept as the original values with a probability\n",
      "of 90% and 10%, respectively. We apply an FC layer to con-\n",
      "vert the Transformer output of each masked region vk into a\n",
      "vector hθ(vk) of the same dimension with the visual feature\n",
      "f(vk). We use cross-entropy loss CE(gθ(vk),C(vk)) to pre-\n",
      "dict the object category of each masked region vk. We also\n",
      "apply another FC layer to convert the Transformer output\n",
      "of each masked region vk to predict the scores of Kobject\n",
      "Dataset Images Texts Langauges\n",
      "Pre-training Corpus\n",
      "Wikipedia - 101G 100\n",
      "Conceptual Captions [29] 3.3M 3.3M 1\n",
      "Fine-tuning and Evaluation Corpus\n",
      "Multi30K [35] 32K 384K 5\n",
      "MSCOCO [2] [34] [20] 120K 1.5M 3\n",
      "Table 1: Statistics of datasets.\n",
      "classes, which further go through a softmax function to be\n",
      "transformed into a normalized distribution gθ(vk). We take\n",
      "the predicted object category with the highest conﬁdence\n",
      "score outputted by Faster-RCNN as the ground-truth label of\n",
      "vk, and convert it into a one-hot vectorC(vk) ∈RK. Due to\n",
      "the top-1 category predicted by Faster-RCNN is not always\n",
      "correct, we leave minimizing the KL divergence between\n",
      "two distributions for our future work. The loss function can\n",
      "be deﬁned as:\n",
      "LMC−MRM(θ) =−E(w[·],v)∼D\n",
      "∑\n",
      "k\n",
      "[MSE(hθ(vk),f(vk))+\n",
      "CE(gθ(vk),C(vk))]\n",
      "where k enumerates the index of each masked image re-\n",
      "gion and MSE(hθ(vk),f(vk)) denotes the mean-square-\n",
      "error loss that regresses the Transformer output of each\n",
      "masked region vk to its visual feature f(vk).\n",
      "Multimodal Code-switched Visual-Linguistic Matching\n",
      "(MC-VLM) This task aims to learn alignment between\n",
      "multilingual texts and images with mixed data stream D. An\n",
      "FC layer sθ(w[·],v) is applied on the Transformer output of\n",
      "[CLS] to predict whether the input image v and the input\n",
      "English or Code-switched text w[·] are semantically matched.\n",
      "Negative image-caption pairs are created by replacing the\n",
      "image or text in a matched sample with a randomly-selected\n",
      "image or text from other samples. We use Binary Cross-\n",
      "Entropy as the loss function:\n",
      "LMC−VLM(θ) =−E(w[·],v)∼D[BCE(sθ(w[·],v),y)]\n",
      "where y∈{0,1}indicates whether the input image-text pair\n",
      "is matched and BCE indicates binary-cross-entropy loss.\n",
      "4. Experiments\n",
      "In this section, we describe detailed experimental settings\n",
      "during pre-training, ﬁne-tuning and evaluating M3P model.\n",
      "4.1. Dataset Description\n",
      "As shown in Table 1, we construct our pre-training dataset\n",
      "based on multimodal corpus, Conceptual Captions [29], and\n",
      "4\n",
      "multilingual corpus, Wikipedia. We evaluate M3P on multi-\n",
      "lingual image-text retrieval task on two datasets: Multi30K\n",
      "[9, 8] and MSCOCO [2, 25, 20]. Panlex† is used as the bilin-\n",
      "gual dictionary during Multimodal Code-switched Training.\n",
      "4.1.1 Pre-training Corpus\n",
      "Conceptual Captions We use Conceptual Captions [29]\n",
      "as the multimodal corpus. It contains 3.3 million English\n",
      "image-caption pairs harvested from the Web and does not\n",
      "contain any non-English text.\n",
      "Wikipedia We use sentences extracted from the Wikipedia\n",
      "dump as the multilingual corpus. It includes 101G sentences\n",
      "covering 100 languages without any vision information.\n",
      "4.1.2 Fine-tuning and Evaluation Corpus\n",
      "Multi30K This dataset extended Flickr30K [35] from En-\n",
      "glish (en) to German (de), French (fr) and Czech (cs). It\n",
      "contains 31,783 images and provides ﬁve captions per image\n",
      "in English and German and one caption per image in French\n",
      "and Czech. The train, dev, and test splits are deﬁned in [35].\n",
      "MSCOCO This dataset contains 123,287 images and pro-\n",
      "vides ﬁve captions per image in English ( en), but fewer\n",
      "in Chinese (zh) and Japanese (ja). STAIR Captions [34]\n",
      "extended MSCOCO [2] with 820K Japanese captions for\n",
      "COCO images. [20] extended MSCOCO [2] with Chinese\n",
      "captions for 20K images. We use the same train, dev, and\n",
      "test splits for English and Japanese as deﬁned in [14]. As for\n",
      "Chinese, we use the COCO-CN split [20].\n",
      "4.1.3 Code-switched Dictionary\n",
      "The word-level bilingual dictionaries used by Code-switched\n",
      "training are from Panlex, the world’s largest open-source\n",
      "lexical translation database. We extract top 50 scale English\n",
      "to other language bilingual dictionaries.\n",
      "4.2. Training Details\n",
      "Pre-training Details Similar to previous vision-language\n",
      "pre-trained models, the M 3P model uses the same model\n",
      "architecture as BERT [7]. We initialize M3P with XLM-R\n",
      "[4] and continue pre-training on our data. We use the same\n",
      "vocabulary as XLM-R [4], which includes 250K BPE tokens\n",
      "and covers 100 languages. We set the dropout rate to 0.1\n",
      "and the max input length to 128. We use Adam Optimizer\n",
      "[16] with a linear warm-up [ 30] and set the learning rate\n",
      "to 1 ×10−4. The total batch size is 1,024 after gradient\n",
      "accumulation. The pre-training stage takes about seven days\n",
      "†https://panlex.org\n",
      "to converge on 8 V100 GPUs. We use Multimodal Code-\n",
      "switched Training with all top 50 languages from Panlex.\n",
      "Fine-tuning Details The batch size is set to 512, and we\n",
      "sample three negative cases for each positive case in VLM.\n",
      "We experiment with different numbers of negative samples\n",
      "in {1,3,5}, and ﬁnd three yields the best results. We use\n",
      "Adam Optimizer with β1 = 0.9, β2 = 0.98 and 5 ×10−5 as\n",
      "the hyper-parameters of learning rate.\n",
      "4.3. Baselines\n",
      "We compare our work with several related work [15, 31,\n",
      "11, 32, 1], which are trained on downstream task datasets\n",
      "(MSCOCO and Multi30K) directly without pre-training. In\n",
      "addition, to make the comparison as fair as possible, we take\n",
      "Unicoder-VL as another baseline, as it employs the same\n",
      "pre-training data during image-language pre-training.\n",
      "Among the baselines, SMALR [1] uses machine transla-\n",
      "tion to augment Multi30K and MSCOCO. But considering\n",
      "that applying machine translation to translate English to all\n",
      "other supported languages lacks generalization and requires\n",
      "a large amount of translators, we leave this as an option for\n",
      "future work. Moreover, note that MULE is using different\n",
      "dev/test splits of MSCOCO compared with other models.\n",
      "It is also worth noticing that word-level dictionaries are\n",
      "only used in M3P, as theMultimodal Code-switched Training\n",
      "is ﬁrstly used in multilingual multimodal pre-training.\n",
      "4.4. Evaluation Settings\n",
      "Multilingual image-text retrieval is the task of ﬁnding the\n",
      "most relevant images given input texts in different languages,\n",
      "or vice versa. We use mean Recall (mR) as our metric, which\n",
      "is an averaged score of Recall@1, Recall@5, and Recall@10\n",
      "on image-to-text retrieval and text-to-image retrieval tasks.\n",
      "We compare M3P with baseline methods on multilingual\n",
      "image-text retrieval in four different settings:\n",
      "(i) w/o ﬁne-tune: apply M 3P to all test sets directly to\n",
      "obtain the evaluation results without ﬁne-tuning.\n",
      "(ii) w/ ﬁne-tune on en : ﬁne-tune M 3P on English and\n",
      "then apply the ﬁne-tuned model to all test sets.\n",
      "(iii) w/ ﬁne-tune on each : ﬁne-tune M 3P on each lan-\n",
      "guage and apply each model to the test set of this language.\n",
      "(iv) w/ ﬁne-tune on all: ﬁne-tune M3P for all languages\n",
      "using the merged labeled data and then apply the ﬁne-tuned\n",
      "model to all test sets.\n",
      "5. Results and Analysis\n",
      "In this section, we show the evaluation results of M 3P\n",
      "compared with existing work and conduct ablation studies\n",
      "in order to better understand the effect of the model.\n",
      "5\n",
      "Model Multi30K MSCOCO\n",
      "en de fr cs en ja zh\n",
      "Monolingual supervised results\n",
      "EmbN [31] 72.0 60.3 54.8 46.3 76.8 73.2 73.5\n",
      "PAR. EmbN [11] 69.0 62.6 60.6 54.1 78.3 76.0 74.8\n",
      "S-LIWE [32] 76.3 72.1 63.4 59.4 80.9 73.6 70.0\n",
      "MULE [15] 70.3 64.1 62.3 57.7 79.0 75.9 75.6\n",
      "SMALR [1] 74.5 69.8 65.9 64.8 81.5 77.5 76.7\n",
      "Monolingual results with multimodal pre-training\n",
      "Unicoder-VL (w/o ﬁne-tune) [19] 72.0 - - - 63.7 - -\n",
      "Unicoder-VL (w/ ﬁne-tune on en) [19] 88.1 - - - 89.2 - -\n",
      "Multilingual results with multimodal pre-training\n",
      "M3P (w/o ﬁne-tune) 57.9 36.8 27.1 20.4 63.1 33.3 32.3\n",
      "M3P (w/ ﬁne-tune on en) 87.4 58.5 46.0 36.8 88.6 53.8 56.0\n",
      "M3P (w/ ﬁne-tune on each) 87.4 82.1 67.3 65.0 88.6 80.1 75.8\n",
      "M3P (w/ ﬁne-tune on all) 87.7 82.7 73.9 72.2 88.7 87.9 86.2\n",
      "Table 2: Multilingual image-text retrieval results on Multi30K and MSCOCO. The metric is the mean Recall (mR). Each bold\n",
      "number indicates the best mR score in that column. We report the mR results of Unicoder-VL on the English test set, as it is\n",
      "pre-trained based on the same image-caption corpus (i.e., Conceptual Captions) with M3P .\n",
      "5.1. Overall Results\n",
      "From Table 2, we have several observations: (1) Our M3P\n",
      "model obtains the state-of-the-art results in all non-English\n",
      "languages, which shows its exciting multilingual multimodal\n",
      "transfer capability. (2) Similar to the observations reported\n",
      "in Unicoder [13, 22], the two fully-supervised settings (iii)\n",
      "w/ ﬁne-tune on each and (iv) w/ ﬁne-tune on all can lead to\n",
      "the best results. This means the same sentence in different\n",
      "languages may capture complementary information to help\n",
      "improve performance. (3) Comparing to Unicoder-VL that is\n",
      "pre-trained using English image-caption corpus (i.e. Concep-\n",
      "tual Captions) only, M3P performs worse on the English test\n",
      "set. The possible reason could be that, M3P needs to balance\n",
      "its multilingual capability over 100+ languages, rather than\n",
      "on English only. (4) In both setting (i) w/o ﬁne-tune and\n",
      "setting (ii) w/ ﬁne-tune on en, integrating Multimodal Code-\n",
      "switched Training (MCT) into M 3P can bring signiﬁcant\n",
      "gains on non-English datasets, which demonstrates good\n",
      "multilingual transfer ability of Multimodal Code-switched\n",
      "Training in the zero-shot setting. It is expected to see such\n",
      "gains become smaller in setting (iii) w/ ﬁne-tune on each and\n",
      "setting (iv) w/ ﬁne-tune on all, as M3P can learn alignments\n",
      "between images and languages from labeled data directly.\n",
      "5.2. Ablation Studies\n",
      "Although we achieve good results under different settings,\n",
      "we want to deep dive into more aspects of M3P: (1) whether\n",
      "Multimodal Code-switched Training (MCT) can provide a\n",
      "positive effect under all settings; (2) whether the number of\n",
      "languages used in MCT affects the performance; (3) whether\n",
      "different pre-training tasks affect the performance.\n",
      "5.2.1 The Impact of MCT\n",
      "To verify whether the Multimodal Code-switched Training\n",
      "(MCT) strategy can provide a positive effect in different\n",
      "settings, we compare the performance of M3P without MCT\n",
      "and M3P with MCT under all ﬁne-tuning settings.\n",
      "For each setting in Table 3, we observe: (1) MCT im-\n",
      "proves the performance on almost all languages, which\n",
      "shows its exciting robustness and expansibility, and (2) in\n",
      "both setting (i) and setting (ii), integrating MCT into M3P\n",
      "can bring signiﬁcant gains on non-English datasets, which\n",
      "demonstrates the good multilingual transferability of MCT.\n",
      "It is expected to see such gains become smaller in settings\n",
      "(iii) and (iv), as M3P can learn alignments between images\n",
      "and languages from labeled data directly.\n",
      "5.2.2 The Impact of Number of Languages in MCT\n",
      "To verify whether the number of languages inﬂuences the\n",
      "performance of Multimodal Code-switched Training (MCT),\n",
      "we conduct an experiment by pre-training M 3P by MCT\n",
      "with different numbers of languages and evaluate the model\n",
      "directly without ﬁne-tuning. We pre-train M 3P with the\n",
      "following settings: pre-train M 3P without MCT, pre-train\n",
      "M3P with MCT on 3 languages (de, fr, cs), 5 languages\n",
      "(de, fr, cs, ja, zh), and all 50 languages.\n",
      "6\n",
      "Setting Multi30K\n",
      "en de fr cs\n",
      "w/o ﬁne-tune\n",
      "M3P w/o MCT 54.9 28.9 25.2 13.5\n",
      "w/ MCT 57.9 36.8 27.1 20.4\n",
      "w/ ﬁne-tune on en\n",
      "M3P w/o MCT 86.0 48.6 37.1 34.6\n",
      "w/ MCT 87.4 58.5 46.0 36.8\n",
      "w/ ﬁne-tune on each\n",
      "M3P w/o MCT 86.0 80.2 67.1 66.2\n",
      "w/ MCT 87.4 82.1 67.3 65.0\n",
      "w/ ﬁne-tune on all\n",
      "M3P w/o MCT 86.7 82.0 73.5 70.2\n",
      "w/ MCT 87.7 82.7 73.9 72.2\n",
      "Table 3: The impact of MCT for multilingual image-text\n",
      "retrieval. The metric is the mean Recall (mR). Each bold\n",
      "number indicates the best mR score.\n",
      "Setting Multi30K\n",
      "en de fr cs\n",
      "M3P w/o MCT 54.9 28.9 25.2 13.5\n",
      "w/ 3 languages MCT 56.4 37.1 28.7 23.0\n",
      "w/ 5 languages MCT 58.2 36.7 26.9 23.6\n",
      "w/ 50 languages MCT (Full) 57.9 36.8 27.1 20.4\n",
      "Table 4: Impact of number of languages inMultimodal Code-\n",
      "switched Training (MCT). The metric is the mean Recall\n",
      "(mR). \"Full\" represents the model pre-trained with all Code-\n",
      "switching languages.\n",
      "In Table 4, we can ﬁnd that, for languages likede and fr,\n",
      "there is no signiﬁcant difference under different settings. On\n",
      "the contrary, for languages likeen and cs, M3P achieves the\n",
      "best performance when MCT is activated with 5 languages.\n",
      "This implies that activating MCT on more languages can\n",
      "lead to more noise due to a higher probability of inaccurate\n",
      "translation. This noise may improve the robustness of the\n",
      "model but make the model harder to be well-trained.\n",
      "5.2.3 The Impact of Proposed Tasks\n",
      "We want to ﬁnd whether each component during pre-training\n",
      "positively affects the performance and try to explain how\n",
      "they gain the performance by conducting several ablation\n",
      "experiments. Since Multimodal Code-switched Training\n",
      "(MCT) inﬂuences each task’s target, we conduct the ablation\n",
      "experiments on M3P without MCT and ﬁne-tune each model\n",
      "on the dataset of each language to compare the performance.\n",
      "Setting Multi30K\n",
      "en de fr cs\n",
      "M3P 86.0 80.2 67.1 66.2\n",
      "w/o xMLM 79.6 70.8 56.4 54.3\n",
      "w/o MC-MLM 84.3 76.2 64.1 62.2\n",
      "w/o MC-MRM 85.5 77.9 65.0 63.9\n",
      "w/o MC-VLM 75.4 68.3 52.7 50.9\n",
      "Table 5: Ablation study on multilingual image-text retrieval.\n",
      "The metric is the mean Recall (mR). Each bold number\n",
      "indicates the best mR score in that column.\n",
      "As shown in Table 5, we can observe that: (1) MC-VLM\n",
      "provides the most considerable improvement (+10.6 on en)\n",
      "to the model among all four sub-tasks during the pre-training\n",
      "stage. We suggest this is because the MC-VLM sub-task\n",
      "successfully models the relationship between image and text.\n",
      "(2) xMLM shows a great impact on non-English results com-\n",
      "pared with English results, which shows that xMLM will\n",
      "improve the capability of multilinguality. (3) MC-MLM and\n",
      "MC-MRM also show good support to the results in all lan-\n",
      "guages, which we suggest these two tasks will help the model\n",
      "learn the knowledge of multimodality. (4) When combining\n",
      "all tasks, we obtain the highest gain in all languages.\n",
      "5.3. Expanding MCT to Fine-tuning\n",
      "Setting Multi30K\n",
      "en de fr cs\n",
      "Pre-trained without MCT\n",
      "M3P (w/ Normal Fine-tune) 86.0 48.6 37.1 34.6\n",
      "M3P (w/ MCT Fine-tune) 85.4 67.8 59.2 54.0\n",
      "Pre-trained with MCT\n",
      "M3P (w/ Normal Fine-tune) 87.4 58.5 46.0 36.8\n",
      "M3P (w/ MCT Fine-tune) 86.4 71.8 62.3 59.6\n",
      "Table 6: The results of expanding MCT to ﬁne-tuning for\n",
      "multilingual image-text retrieval. The metric is the mean\n",
      "Recall (mR). Each bold numberindicates the best mR score\n",
      "under the setting. Normal Fine-tune represents ﬁne-tuning\n",
      "with English data directly and MCT Fine-tune represents\n",
      "ﬁne-tuning with Code-switched English data.\n",
      "Similar to MC-VLM, we use Code-switched data to ﬁne-\n",
      "tune M3P on Multi30K. The results in Table 6 show: (1)\n",
      "Multimodal Code-switched Training(MCT) can bring a large\n",
      "margin for non-English language probably because of the\n",
      "lack of labeled image-non English caption pairs during the\n",
      "pre-training stage or ﬁne-tuning stage. (2) Employing MCT\n",
      "into the ﬁne-tuning stage for the model, whatever pre-trained\n",
      "by, will achieve a large increase in non-English performance.\n",
      "7\n",
      "(3) MCT in ﬁne-tuning is more effective than MCT in pre-\n",
      "training, which may be explained by that the model can learn\n",
      "multilinguality in a more speciﬁc task. (4) The best results\n",
      "can be achieved when MCT replaces English in both the\n",
      "pre-training and ﬁne-tuning stages.\n",
      "5.4. Qualitative Studies on MCT\n",
      "To further explore how Multimodal Code-switched Train-\n",
      "ing (MCT) affects the model, we randomly select some\n",
      "text-image pairs generated from it. We want to ﬁgure out\n",
      "why Multimodal Code-switched Trainingis very effective on\n",
      "non-English languages and whether it has any limitations.\n",
      "Figure 2: Qualitative study for Multimodal Code-switched\n",
      "Training (MCT). The ﬁrst row in each table is the original\n",
      "text, and the second row in each table is the Code-switched\n",
      "text. We add the meaning of the Code-switched text in\n",
      "English in the third row of each table.\n",
      "As Figure 2 (a) shows, the meaning of the code-switched\n",
      "text generated by Multimodal Code-switched Training\n",
      "(MCT) is almost the same as that of the original text. Al-\n",
      "though there are some small differences between the original\n",
      "text (ﬁrst row) and the generated text translated back to En-\n",
      "glish (third row), it has no inﬂuence on the training quality,\n",
      "which demonstrates the reason why MCT brings gains. The\n",
      "key idea of using MCT in M3P is to let the model see more\n",
      "Code-switched text and image pairs and learn the joint mul-\n",
      "tilingual multimodal representations from such pairs directly.\n",
      "We guess this helps the model learn richer information of\n",
      "each token from the multilingual context.\n",
      "We did not consider the grammar or syntax correctness of\n",
      "the Code-switched sentences generated by replacing words\n",
      "in the English sentences with their word translations in\n",
      "other languages. The pre-trained models can learn such\n",
      "knowledge from well-formed multilingual sentences and En-\n",
      "glish caption sentences. Since we don’t have image-caption\n",
      "pairs or high-quality machine translation engines to generate\n",
      "such data for most languages, generating Code-switched sen-\n",
      "tences is the most effective way to let M3P directly see more\n",
      "alignments between non-English languages and images.\n",
      "Hence, because of the high accuracy of translation from\n",
      "MCT, multilingual results will signiﬁcantly increase when\n",
      "no non-English multimodal data is available. However, when\n",
      "the model can access high-quality multilingual multimodal\n",
      "data, the noise from MCT may limit its performance. In\n",
      "Figure 2 (b), we show a negative case in Code-switched text.\n",
      "MCT faultily changes the meaning of the original text. We\n",
      "leave this as future work to solve this problem.\n",
      "6. Conclusion\n",
      "We have presented in this paper a new pre-trained model\n",
      "M3P which combines Multilingual Pre-training and Multi-\n",
      "modal Pre-training into a uniﬁed framework via Multitask\n",
      "Pre-training for multilingual multimodal scenarios. We pro-\n",
      "posed Multimodal Code-switched Training to further allevi-\n",
      "ate the issue of lacking enough labeled data for non-English\n",
      "multimodal tasks and avoid the tendency to model the rela-\n",
      "tionship between vision and English text.\n",
      "References\n",
      "[1] Andrea Burns, Donghyun Kim, Derry Wijaya, Kate Saenko,\n",
      "and Bryan A Plummer. Learning to scale multilingual\n",
      "representations for vision-language tasks. arXiv preprint\n",
      "arXiv:2004.04312, 2020. 5, 6\n",
      "[2] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\n",
      "tam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick.\n",
      "Microsoft coco captions: Data collection and evaluation\n",
      "server. arXiv preprint arXiv:1504.00325, 2015. 2, 4, 5\n",
      "[3] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\n",
      "Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\n",
      "Learning universal image-text representations. arXiv preprint\n",
      "arXiv:1909.11740, 2019. 1, 2\n",
      "[4] Alexis Conneau, Kartikay Khandelwal, Naman Goyal,\n",
      "Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán,\n",
      "Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin\n",
      "Stoyanov. Unsupervised cross-lingual representation learning\n",
      "at scale. arXiv preprint arXiv:1911.02116, 2019. 1, 5\n",
      "[5] Alexis Conneau and Guillaume Lample. Cross-lingual lan-\n",
      "guage model pretraining. In Advances in Neural Information\n",
      "Processing Systems, pages 7057–7067, 2019. 1, 2, 3\n",
      "[6] Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina\n",
      "Williams, Samuel R Bowman, Holger Schwenk, and Veselin\n",
      "Stoyanov. Xnli: Evaluating cross-lingual sentence representa-\n",
      "tions. arXiv preprint arXiv:1809.05053, 2018. 2\n",
      "[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova. Bert: Pre-training of deep bidirectional transform-\n",
      "ers for language understanding. In Proceedings of the 2019\n",
      "8\n",
      "Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics, pages 4171–4186, 2019. 1, 2,\n",
      "3, 5\n",
      "[8] Desmond Elliott, Stella Frank, Loïc Barrault, Fethi Bougares,\n",
      "and Lucia Specia. Findings of the second shared task on\n",
      "multimodal machine translation and multilingual image de-\n",
      "scription. arXiv preprint arXiv:1710.07177, 2017. 5\n",
      "[9] Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia\n",
      "Specia. Multi30k: Multilingual english-german image de-\n",
      "scriptions. arXiv preprint arXiv:1605.00459, 2016. 5\n",
      "[10] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,\n",
      "and Jingjing Liu. Large-scale adversarial training for\n",
      "vision-and-language representation learning. arXiv preprint\n",
      "arXiv:2006.06195, 2020. 1\n",
      "[11] Spandana Gella, Rico Sennrich, Frank Keller, and Mirella\n",
      "Lapata. Image pivoting for learning multilingual multimodal\n",
      "representations. In: Empirical Methods in Natural Language\n",
      "Processing (EMNLP) (2017), 2017. 5, 6\n",
      "[12] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr\n",
      "Dollár, and Kaiming He. Detectron. 2018. 3\n",
      "[13] Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Lin-\n",
      "jun Shou, Daxin Jiang, and Ming Zhou. Unicoder: A universal\n",
      "language encoder by pre-training with multiple cross-lingual\n",
      "tasks. arXiv preprint arXiv:1909.00964, 2019. 1, 2, 3, 6\n",
      "[14] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\n",
      "ments for generating image descriptions. In Proceedings of\n",
      "the IEEE conference on computer vision and pattern recogni-\n",
      "tion, pages 3128–3137, 2015. 5\n",
      "[15] Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff,\n",
      "and Bryan A Plummer. Mule: Multimodal universal language\n",
      "embedding. In: AAAI Conference on Artiﬁcial Intelligence\n",
      "(2020), 2020. 5, 6\n",
      "[16] Diederik P Kingma and Jimmy Ba. Adam: A method for\n",
      "stochastic optimization. international conference on learning\n",
      "representations, 2015. 5\n",
      "[17] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\n",
      "Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\n",
      "tidis, Li-Jia Li, David A Shamma, et al. Visual genome:\n",
      "Connecting language and vision using crowdsourced dense\n",
      "image annotations. International Journal of Computer Vision,\n",
      "123(1):32–73, 2017. 2\n",
      "[18] Taku Kudo and John Richardson. Sentencepiece: A simple\n",
      "and language independent subword tokenizer and detokenizer\n",
      "for neural text processing. EMNLP, 2018. 2\n",
      "[19] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming\n",
      "Zhou. Unicoder-vl: A universal encoder for vision and lan-\n",
      "guage by cross-modal pre-training. AAAI, 2020. 1, 2, 4,\n",
      "6\n",
      "[20] Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengx-\n",
      "iong Jia, Gang Yang, and Jieping Xu. Coco-cn for cross-\n",
      "lingual image tagging, captioning and retrieval. In IEEE\n",
      "Transactions on Multimedia, 2019. 4, 5\n",
      "[21] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan\n",
      "Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\n",
      "Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics\n",
      "aligned pre-training for vision-language tasks. arXiv preprint\n",
      "arXiv:2004.06165, 2020. 1, 2\n",
      "[22] Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo,\n",
      "Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong\n",
      "Cao, et al. Xglue: A new benchmark dataset for cross-lingual\n",
      "pre-training, understanding and generation. arXiv preprint\n",
      "arXiv:2004.01401, 2020. 6\n",
      "[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\n",
      "Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\n",
      "moyer, and Veselin Stoyanov. Roberta: A robustly optimized\n",
      "bert pretraining approach. arXiv preprint arXiv:1907.11692,\n",
      "2019. 1\n",
      "[24] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\n",
      "Pretraining task-agnostic visiolinguistic representations for\n",
      "vision-and-language tasks. In Advances in Neural Information\n",
      "Processing Systems, pages 13–23, 2019. 2, 4\n",
      "[25] Takashi Miyazaki and Nobuyuki Shimizu. Cross-lingual\n",
      "image caption generation. In ACL, 2016. 5\n",
      "[26] Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.\n",
      "Im2text: Describing images using 1 million captioned pho-\n",
      "tographs. In Advances in neural information processing sys-\n",
      "tems, pages 1143–1151, 2011. 2\n",
      "[27] Libo Qin, Minheng Ni, Yue Zhang, and Wanxiang Che.\n",
      "Cosda-ml: Multi-lingual code-switching data augmen-\n",
      "tation for zero-shot cross-lingual nlp. arXiv preprint\n",
      "arXiv:2006.06402, 2020. 2, 3\n",
      "[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\n",
      "Amodei, and Ilya Sutskever. Language models are unsuper-\n",
      "vised multitask learners. OpenAI blog, 1(8):9, 2019. 1\n",
      "[29] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\n",
      "Soricut. Conceptual captions: A cleaned, hypernymed, im-\n",
      "age alt-text dataset for automatic image captioning. In Pro-\n",
      "ceedings of the 56th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers), pages\n",
      "2556–2565, 2018. 2, 4, 5\n",
      "[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\n",
      "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\n",
      "Polosukhin. Attention is all you need. In Advances in neural\n",
      "information processing systems, pages 5998–6008, 2017. 5\n",
      "[31] Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazebnik.\n",
      "Learning two-branch neural networks for image-text match-\n",
      "ing tasks. IEEE Transactions on Pattern Analysis and Ma-\n",
      "chine Intelligence, 41(2):394–407, 2018. 5, 6\n",
      "[32] Jônatas Wehrmann, Douglas M Souza, Mauricio A Lopes,\n",
      "and Rodrigo C Barros. Language-agnostic visual-semantic\n",
      "embeddings. In Proceedings of the IEEE International Con-\n",
      "ference on Computer Vision, pages 5804–5813, 2019. 5, 6\n",
      "[33] Zhen Yang, Bojie Hu, Ambyera Han, Shen Huang, and Qi Ju.\n",
      "Csp: Code-switching pre-training for neural machine trans-\n",
      "lation. In Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP), pages\n",
      "2624–2636, 2020. 2\n",
      "[34] Yuya Yoshikawa, Yutaro Shigeto, and Akikazu Takeuchi.\n",
      "Stair captions: Constructing a large-scale japanese image\n",
      "caption dataset. arXiv preprint arXiv:1705.00823, 2017. 4, 5\n",
      "[35] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\n",
      "maier. From image descriptions to visual denotations: New\n",
      "similarity metrics for semantic inference over event descrip-\n",
      "tions. Transactions of the Association for Computational\n",
      "Linguistics, 2:67–78, 2014. 4, 5\n",
      "9\n",
      "[36] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu,\n",
      "and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-\n",
      "language representations through scene graph. arXiv preprint\n",
      "arXiv:2006.16934, 2020. 1\n",
      "[37] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\n",
      "son J Corso, and Jianfeng Gao. Uniﬁed vision-language\n",
      "pre-training for image captioning and vqa. AAAI, 2020. 2\n",
      "10\n",
      "The following is a summary of the paper: M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training\n",
      "\n",
      "Summary: We present M3P, a Multitask Multilingual Multimodal Pre-trained model that\n",
      "combines multilingual pre-training and multimodal pre-training into a unified\n",
      "framework via multitask pre-training. Our goal is to learn universal\n",
      "representations that can map objects occurred in different modalities or texts\n",
      "expressed in different languages into a common semantic space. In addition, to\n",
      "explicitly encourage fine-grained alignment between images and non-English\n",
      "languages, we also propose Multimodal Code-switched Training (MCT) to combine\n",
      "monolingual pre-training and multimodal pre-training via a code-switch\n",
      "strategy. Experiments are performed on the multilingual image retrieval task\n",
      "across two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve\n",
      "comparable results for English and new state-of-the-art results for non-English\n",
      "languages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from openai import OpenAI\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "import pprint\n",
    "import json\n",
    "import random\n",
    "from fundus import PublisherCollection, Crawler\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.readers.papers import ArxivReader\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://20.81.188.27:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "\n",
    "\n",
    "def load_arxiv_papers(search_query: str, max_results: int = 1):    \n",
    "    loader = ArxivReader()\n",
    "    documents = loader.load_data(search_query=search_query, max_results=max_results)\n",
    "    return documents\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-Large-Instruct-2411\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "documents = load_arxiv_papers(\"(multimodality OR multitask learning) AND ((low-resource languages) OR (code-switching)) -SoupLM\", max_results=1)\n",
    "\n",
    "for document in documents:\n",
    "    print(document.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2b268fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='a44d6c99-896d-4657-8b84-5d9016aa665f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The following is a summary of the paper: M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training\\n\\nSummary: We present M3P, a Multitask Multilingual Multimodal Pre-trained model that\\ncombines multilingual pre-training and multimodal pre-training into a unified\\nframework via multitask pre-training. Our goal is to learn universal\\nrepresentations that can map objects occurred in different modalities or texts\\nexpressed in different languages into a common semantic space. In addition, to\\nexplicitly encourage fine-grained alignment between images and non-English\\nlanguages, we also propose Multimodal Code-switched Training (MCT) to combine\\nmonolingual pre-training and multimodal pre-training via a code-switch\\nstrategy. Experiments are performed on the multilingual image retrieval task\\nacross two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve\\ncomparable results for English and new state-of-the-art results for non-English\\nlanguages.', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "49986c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-fc639abe4307403fa9a0e3800450479c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=' \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{\\n\"question1\": \"What is the primary goal of multimodal pre-training in language models?\",\\n\"answer1\": \"The primary goal of multimodal pre-training in language models is to learn universal representations that can map objects occurring in different modalities, such as images and text, into a common semantic space. This enables the model to understand and relate information across different types of inputs.\"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t,\\n\"question2\":\"What is the purpose of multilingual pre-training in language models?\",\\n\"answer2\": \"The purpose of multilingual pre-training in language models is to enable the model to understand and generate text in multiple languages. This involves learning representations that can capture the semantics of text expressed in different languages and mapping them into a shared semantic space.\"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t,\\n\"question3\":\"What is Multimodal Code-switched Training (MCT) and why is it used?\",\\n\"answer3\":\"Multimodal Code-switched Training (MCT) is a strategy used to combine monolingual pre-training and multimodal pre-training in language models. It involves switching between different languages and modalities during training. MCT is used to explicitly encourage fine-grained alignment between non-English languages and other modalities, such as images, improving the model\\'s performance in multilingual and multimodal tasks.\"\\n}\\n\\n   \\t \\t   \\t \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t \\t  \\t  \\t     \\t  \\t      \\t \\t \\t         \\t \\t    \\t  \\t \\t    \\t \\t\\n \\t \\t \\t    \\t  \\t \\t  \\t  \\t   \\n\\n\\n   \\t \\t   \\t \\t \\t    \\t  \\t \\t  \\t  \\t   \\r \\t  \\t      \\t \\t  \\t  \\t     \\t  \\t  \\t   \\n\\n\\n   \\t \\t   \\t \\t  \\t  \\t   \\n \\t \\t    \\t  \\t \\t  \\t      \\t \\n\\n   \\t \\t   \\n\\n   \\t \\t   \\t \\t  \\t      \\t \\t  \\t  \\t     \\t  \\t  \\t   \\n\\n\\n   \\t \\t           \\t  \\t \\n\\n   \\t \\r\\n\\n  \\n\\n   \\r\\n\\n         \\r \\t  \\t \\n\\n         \\r       \\t  \\t \\n\\n   \\r\\r    \\r      \\r           \\r           \\r    \\r      \\r    \\r      \\r    \\r      \\r    \\r      \\r           \\r           \\r           \\r    \\r      \\r           \\r           \\r           \\r           \\r    \\r      \\r           \\r           \\r    \\r      \\r    \\r      \\r    \\r      \\r    \\r      \\r    \\r      \\r           \\r           \\r           \\r    \\r      \\r    \\r      \\r    \\r      \\r    \\r        \\r\\n          \\r\\n             \\r          \\r\\n           \\r          \\r\\r          \\r\\n                                  \\r\\n                       \\r                       \\r           \\r         \\r           \\r\\r                     \\r\\r          \\r\\r \\r        \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r           \\r\\r         \\r\\r             \\r           \\r           \\r          \\r\\n           \\r           \\r          \\r\\n           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r          \\r\\n                                   \\r           \\r           \\r          \\r\\n                                  \\r\\n           \\r          \\r\\n                                  \\r\\n           \\r          \\r\\n                                  \\r\\n           \\r          \\r\\n                                  \\r\\n           \\r          \\r\\n                                  \\r\\n           \\r          \\r\\n                                  \\r\\n           \\r          \\r\\r          \\r\\n           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r          \\r\\n                                  \\n\\n           \\r           \\n                                 \\r\\r\\n           \\r          \\r\\r          \\r\\n           \\r           \\n                                 \\r\\r\\n           \\r          \\r\\r          \\r\\n           \\r          \\r\\r         \\r\\r\\n                                 \\r\\r\\n           \\r           \\n                                 \\r\\r\\n           \\r          \\r\\r          \\r\\n           \\r          \\r\\r         \\r\\r\\n                                 \\r\\r\\n           \\r          \\r\\n                                 \\r\\r\\n           \\r           \\n                                 \\r\\r\\n           \\r          \\r\\r         \\r\\r\\n                                 \\r\\r\\n           \\r          \\r\\n                                 \\r\\r\\n           \\r          \\r\\r          \\r\\n           \\r           \\n                                 \\r\\r\\n           \\r           \\n                                 \\r\\r\\n           \\r          \\r\\n                                 \\r \\t       \\r   \\r       \\r                                                   \\r           \\r                   \\r   \\r           \\r           \\r       \\r           \\r               \\r       \\r   \\r       \\r\\r          \\r   \\r       \\r            \\r                      \\r           \\r\\r          \\r   \\r        \\r                      \\r   \\r       \\r\\r          \\r   \\r       \\r   \\r        \\r                      \\r           \\r\\r          \\r   \\r       \\r   \\r        \\r                      \\r   \\r       \\r\\r          \\r   \\r       \\r            \\r                      \\r           \\r\\r          \\r           \\r   \\r        \\r                      \\r           \\r\\r          \\r           \\r   \\r        \\r                      \\r   \\r       \\r\\r          \\r           \\r            \\r                      \\r   \\r       \\r\\r          \\r           \\r            \\r                      \\r           \\r\\r          \\r   \\r       \\r            \\r                     \\r    \\r       \\r\\r         \\r    \\r       \\r                     \\r\\r            \\r                     \\r\\r            \\r                     \\r\\r    \\r       \\r                     \\r\\r    \\r       \\r                     \\r\\r            \\r                     \\r\\r    \\r       \\r                     \\r\\r    \\r       \\r                     \\r\\r            \\r                     \\r\\r            \\r                     \\r\\r            \\r                     \\r\\r    \\r       \\r                     \\r\\r            \\r                     \\r\\r    \\r       \\r                     \\r\\r    \\r       \\r                     \\r\\r    \\r       \\r                     \\r\\r    \\r       \\r                     \\r\\r    \\r       \\r                     \\r\\r            \\r                     \\r\\r    \\r       \\r                     \\r\\r    \\r       \\r                       \\t \\t \\t \\t \\t \\t', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1734919609, model='/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=2001, prompt_tokens=392, total_tokens=2393, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QA(BaseModel):\n",
    "    question1: str = Field(description=\"First general knowledge question about the core topic and concepts\")\n",
    "    answer1: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "    question2: str = Field(description=\"Second general knowledge question about the core topic and concepts\") \n",
    "    answer2: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "    question3: str = Field(description=\"Third general knowledge question about the core topic and concepts\")\n",
    "    answer3: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "\n",
    "DEFAULT_SYSTEM_CONTENT = \"\"\"You are an expert at creating high-quality training data for language models.\n",
    "Given academic research content, extract the key concepts and convert them into general knowledge questions and answers.\n",
    "Questions should:\n",
    "- Be general and broadly applicable\n",
    "- Focus on core concepts and ideas\n",
    "- Be written as standalone questions without referencing any specific research\n",
    "- Test understanding of the topic area\n",
    "\n",
    "Answers should:\n",
    "- Provide clear, factual information\n",
    "- Be written as standalone knowledge\n",
    "- Focus on general concepts rather than specific research\n",
    "- Be useful for general learning about the topic\"\"\"\n",
    "\n",
    "def generate_qa(document_text: str, system_content: str = DEFAULT_SYSTEM_CONTENT):\n",
    "    qa_format = QA.schema()\n",
    "    \n",
    "    return client.chat.completions.create(\n",
    "        model=\"/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": f\"Based on these concepts, generate 3 general knowledge questions and answers about: {document_text}\"}\n",
    "        ],\n",
    "        extra_body={\n",
    "            \"min_p\": 0.4,\n",
    "            \"min_tokens\": 2000,\n",
    "            \"guided_json\": qa_format,\n",
    "            \"repetition_penalty\": 1.1\n",
    "        },\n",
    "        temperature=1.5,\n",
    "        max_tokens=4000,\n",
    "        n=1,\n",
    "        presence_penalty=0.1,\n",
    "        frequency_penalty=0.1,\n",
    "        stop=\"<|eot_id|>\"\n",
    "    )\n",
    "\n",
    "qa_response = generate_qa(document.text)\n",
    "qa_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bef6996c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8ab47a0ccca24c05ac89f3e672e213a8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='  {\\n    \"qa_pair1_arguments_for\": [\\n        \"The question directly relates to the main topic of the paper\",\\n        \"The answer accurately reflects the goal of multimodal pre-training as described in the paper\",\\n        \"The pair is clear and concise\"\\n    ],\\n    \"qa_pair1_arguments_against\": [],\\n    \"qa_pair1_include\": \"y\",\\n\\n    \"qa_pair2_arguments_for\": [\\n        \"The question is relevant to the paper\\'s scope\",\\n        \"The answer accurately explains the purpose of multilingual pre-training\",\\n        \"The pair is well-structured and easy to understand\"\\n    ],\\n    \"qa_pair2_arguments_against\": [],\\n    \"qa_pair2_include\": \"y\",\\n\\n    \"qa_pair3_arguments_for\": [\\n        \"The question addresses a specific and novel aspect of the paper\",\\n        \"The answer provides a clear explanation of MCT and its purpose\",\\n        \"Including this pair would help preserve the nuances of the original paper\"\\n    ],\\n    \"qa_pair3_arguments_against\": [\\n        \"The term \\'code-switch\\' might need further clarification\"\\n    ],\\n    \"qa_pair3_include\": \"y\",\\n\\n    \"next_search_query\": \"zero-shot learning in multilingual language models AND evaluation metrics OR cross lingual transfer learning AND low resource scenarios\" }\\n\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t \\t\\t\\t\\t\\t\\t\\t\\t\\t \\t \\t\\t\\t\\t\\t\\t\\t\\t\\t \\t \\t\\t\\t\\t\\t\\t\\t\\t\\t \\t \\t\\t\\t\\t\\t\\t\\t\\t\\t \\t \\t\\t\\t\\t\\t\\t\\t\\t\\t \\t \\t\\t\\t\\t\\t\\t\\t\\t  \\t \\t \\t      \\t      \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t         \\t \\t           \\r   \\r   \\r   \\r   \\r   \\r      \\r\\n  \\r  \\r   \\r\\n\\n  \\r  \\r   \\r\\n\\n           \\r           \\r                       \\r                                               \\r           \\r                       \\r                                   \\r                       \\r                                   \\r                                               \\r           \\r                                   \\r                       \\r           \\r           \\r           \\r                                   \\r           \\r           \\r                                                                       \\r           \\r               \\r\\n    \\r\\n\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n   \\r\\n\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n   \\r\\n\\n    \\r\\n    \\r\\n    \\r\\n   \\r\\n\\n    \\r\\n   \\r\\n\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n   \\r\\n\\n    \\r\\n   \\r\\n\\n   \\r\\r   \\r\\n\\r\\n    \\r\\r   \\r\\n\\r        \\r\\r         \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n         \\r\\r   \\r        \\r\\r        \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r      \\r\\r   \\r                    \\r\\r        \\r             \\r\\r        \\r\\r           \\r\\n          \\r\\n          \\r\\n          \\r\\n         \\r\\r             \\r\\n         \\r\\r             \\r           \\r          \\r            \\r          \\r\\r           \\r          \\r\\r           \\r          \\r\\r           \\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r           \\r          \\r                                                \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r                       \\r         \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n          \\r\\n         \\r\\r          \\r  \\r        \\r  \\r        \\r\\r             \\r        \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r            \\r\\r        \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r\\r          \\r \\r        \\r                       \\r                                   \\r                       \\r\\r                      \\r\\r          \\r           \\r           \\r\\r                      \\r\\r                                  \\r\\r                      \\r\\r                      \\r\\r                      \\r           \\r\\r          \\r\\r          \\r\\r          \\r\\r                                  \\r                                               \\r           \\r                                   \\r           \\r\\r           \\r  \\r         \\r                                     \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r           \\r          \\r                                                \\r                                                                       \\r                       \\r                                               \\r                                               \\r                       \\r                       \\r                       \\r                                               \\r                       \\r                       \\r                                               \\r                                                                       \\r                       \\r                       \\r                                                                                                            ', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1734919715, model='/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=2002, prompt_tokens=743, total_tokens=2745, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QAEvaluation(BaseModel):\n",
    "    qa_pair1_arguments_for: list[str] = Field(description=\"Arguments in favor of including first QA pair\")\n",
    "    qa_pair1_arguments_against: list[str] = Field(description=\"Arguments against including first QA pair\") \n",
    "    qa_pair1_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include first QA pair\")\n",
    "\n",
    "    qa_pair2_arguments_for: list[str] = Field(description=\"Arguments in favor of including second QA pair\")\n",
    "    qa_pair2_arguments_against: list[str] = Field(description=\"Arguments against including second QA pair\")\n",
    "    qa_pair2_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include second QA pair\")\n",
    "\n",
    "    qa_pair3_arguments_for: list[str] = Field(description=\"Arguments in favor of including third QA pair\")\n",
    "    qa_pair3_arguments_against: list[str] = Field(description=\"Arguments against including third QA pair\")\n",
    "    qa_pair3_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include third QA pair\")\n",
    "    \n",
    "    next_search_query: str = Field(description=\"Recommended next search query for finding related but diverse papers\")\n",
    "\n",
    "def evaluate_qa(document_text: str, qa_output: QA):\n",
    "    evaluation_format = QAEvaluation.schema()\n",
    "    \n",
    "    eval_system_prompt = \"\"\"You are an expert evaluator of training data quality for language models.\n",
    "    For each question-answer pair:\n",
    "    1. List key arguments for and against including it (considering accuracy, clarity, generalizability)\n",
    "    2. Make a binary decision (y/n) if it should be included in the final dataset\n",
    "    \n",
    "    Additionally, recommend a search query for finding the next paper to analyze. The query should:\n",
    "    - Be related to but very distinct from the current paper's topic\n",
    "    - Help build a diverse dataset while maintaining topical coherence\n",
    "    - Potentially focus on an interesting direction suggested by the current paper\"\"\"\n",
    "    \n",
    "    eval_user_prompt = f\"\"\"Evaluate these QA pairs generated from the paper:\n",
    "    \n",
    "    Original Paper Text: {document_text}\n",
    "    \n",
    "    QA Pair 1:\n",
    "    Q: {qa_output.question1}\n",
    "    A: {qa_output.answer1}\n",
    "    \n",
    "    QA Pair 2:\n",
    "    Q: {qa_output.question2}\n",
    "    A: {qa_output.answer2}\n",
    "    \n",
    "    QA Pair 3:\n",
    "    Q: {qa_output.question3}\n",
    "    A: {qa_output.answer3}\n",
    "    \n",
    "    Provide a structured evaluation following the schema, including a recommended next search query.\"\"\"\n",
    "    \n",
    "    return client.chat.completions.create(\n",
    "        model=\"/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": eval_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_user_prompt}\n",
    "        ],\n",
    "        extra_body={\n",
    "            \"min_p\": 0.4,\n",
    "            \"min_tokens\": 2000,\n",
    "            \"guided_json\": evaluation_format,\n",
    "            \"repetition_penalty\": 1.1\n",
    "        },\n",
    "        temperature=1.5,\n",
    "        max_tokens=4000,\n",
    "        n=1,\n",
    "        presence_penalty=0.1,\n",
    "        frequency_penalty=0.1,\n",
    "        stop=\"<|eot_id|>\"\n",
    "    )\n",
    "\n",
    "evaluation_response = evaluate_qa(document.text, QA.parse_raw(qa_response.choices[0].message.content))\n",
    "evaluation_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "73851ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎓 WAND UNIVERSITY KNOWLEDGE ACQUISITION SYSTEM INITIALIZED 🎓\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RESEARCH CYCLES PROGRESS:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 SEARCHING ARCHIVES WITH FOCUS: (large language models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📜 ANALYZING MAGICAL MANUSCRIPT:\n",
      "================================================================================\n",
      "\n",
      "Lost in Translation\n",
      "May 2023\n",
      "A report from\n",
      "Gabriel Nicholas \n",
      "Aliya Bhatia\n",
      "Large Language Models in \n",
      "Non-English Content Analysis......\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🔮 INITIATING KNOWLEDGE SYNTHESIS AND EVALUATION 🔮\n",
      "📖 GENERATING ENLIGHTENED KNOWLEDGE EXCHANGES...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def archive_enchanted_dialogues(knowledge_exchange_data, assessment_data, archive_path=\"wand_university_training_grimoire.csv\"):\n",
    "    \"\"\"Archive validated magical dialogues and their scholarly assessments for the Wand University curriculum\"\"\"\n",
    "    print(\"\\n🌟 INITIATING ARCHIVAL PROCESS OF ENCHANTED DIALOGUES 🌟\")\n",
    "    \n",
    "    # Transform assessment data from arcane notation to comprehensible format\n",
    "    assessment_grimoire = json.loads(assessment_data)\n",
    "    \n",
    "    # Initialize collection of validated knowledge exchanges\n",
    "    validated_exchanges = []\n",
    "    \n",
    "    # Evaluate each knowledge exchange through our rigorous magical standards\n",
    "    for exchange_id in tqdm(range(1,4), desc=\"VALIDATING KNOWLEDGE EXCHANGES\"):\n",
    "        if assessment_grimoire[f'qa_pair{exchange_id}_include'] == 'y':\n",
    "            # Synthesize supporting and challenging arguments\n",
    "            supporting_thesis = assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_for', [])\n",
    "            counterpoints = assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_against', [])\n",
    "            \n",
    "            print(f\"\\n✨ ACCEPTED KNOWLEDGE EXCHANGE {exchange_id}:\")\n",
    "            print(f\"📝 INQUIRY: {getattr(knowledge_exchange_data, f'question{exchange_id}')}\")\n",
    "            print(f\"💭 RESPONSE: {getattr(knowledge_exchange_data, f'answer{exchange_id}')}\")\n",
    "            print(\"✅ POSITIVE FEEDBACK:\")\n",
    "            for strength in supporting_thesis:\n",
    "                print(f\"  • {strength}\")\n",
    "            \n",
    "            validated_exchanges.append({\n",
    "                'inquiry': getattr(knowledge_exchange_data, f'question{exchange_id}'),\n",
    "                'wisdom': getattr(knowledge_exchange_data, f'answer{exchange_id}'),\n",
    "                'supporting_thesis': ', '.join(supporting_thesis),\n",
    "                'counterpoints': ', '.join(counterpoints),\n",
    "                'search_query': assessment_grimoire['next_search_query']\n",
    "            })\n",
    "        else:\n",
    "            print(f\"\\n⚠️ DISCARDED KNOWLEDGE EXCHANGE {exchange_id}:\")\n",
    "            print(f\"📝 INQUIRY: {getattr(knowledge_exchange_data, f'question{exchange_id}')}\")\n",
    "            print(f\"💭 RESPONSE: {getattr(knowledge_exchange_data, f'answer{exchange_id}')}\")\n",
    "            print(\"❌ CRITICAL FEEDBACK:\")\n",
    "            for critique in assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_against', []):\n",
    "                print(f\"  • {critique}\")\n",
    "    \n",
    "    print(\"\\n📚 TRANSFORMING KNOWLEDGE INTO STRUCTURED FORMAT...\")\n",
    "    # Transform into structured knowledge format\n",
    "    knowledge_codex = pd.DataFrame(validated_exchanges)\n",
    "    \n",
    "    # Preserve in the grand archives\n",
    "    print(\"💾 PRESERVING IN THE GRAND ARCHIVES...\")\n",
    "    if Path(archive_path).exists():\n",
    "        knowledge_codex.to_csv(archive_path, mode='a', header=False, index=False, sep='|')\n",
    "    else:\n",
    "        knowledge_codex.to_csv(archive_path, index=False, sep='|')\n",
    "    \n",
    "    print(f\"✨ SUCCESSFULLY ARCHIVED {len(validated_exchanges)} EXCHANGES ✨\")\n",
    "    return len(validated_exchanges)\n",
    "\n",
    "def synthesize_and_evaluate_knowledge(source_manuscript, research_focus):\n",
    "    \"\"\"Synthesize and evaluate magical knowledge exchanges through our proprietary arcane processes\"\"\"\n",
    "    print(\"\\n🔮 INITIATING KNOWLEDGE SYNTHESIS AND EVALUATION 🔮\")\n",
    "    \n",
    "    # Generate enlightened knowledge exchanges\n",
    "    print(\"📖 GENERATING ENLIGHTENED KNOWLEDGE EXCHANGES...\")\n",
    "    knowledge_response = generate_qa(source_manuscript, research_focus)\n",
    "    knowledge_data = QA.parse_raw(knowledge_response.choices[0].message.content)\n",
    "    \n",
    "    # Submit for rigorous academic evaluation\n",
    "    print(\"⚖️ CONDUCTING RIGOROUS ACADEMIC EVALUATION...\")\n",
    "    assessment_response = evaluate_qa(source_manuscript, knowledge_data)\n",
    "    assessment_results = assessment_response.choices[0].message.content\n",
    "    \n",
    "    # Archive worthy exchanges\n",
    "    exchanges_preserved = archive_enchanted_dialogues(knowledge_data, assessment_results)\n",
    "    \n",
    "    # Extract next research direction from evaluation\n",
    "    assessment_grimoire = json.loads(assessment_results)\n",
    "    next_research_focus = assessment_grimoire['next_search_query']\n",
    "    \n",
    "    return exchanges_preserved, next_research_focus\n",
    "\n",
    "# Initialize primary research focus\n",
    "current_research_focus = \"(large language models)\"\n",
    "\n",
    "# Initialize arcane metrics\n",
    "preserved_exchanges_count = 0\n",
    "research_cycles = 0\n",
    "maximum_research_cycles = 10  # Adjustable based on arcane energy levels\n",
    "\n",
    "print(\"\\n🎓 WAND UNIVERSITY KNOWLEDGE ACQUISITION SYSTEM INITIALIZED 🎓\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with tqdm(total=maximum_research_cycles, desc=\"RESEARCH CYCLES PROGRESS\") as pbar:\n",
    "    while research_cycles < maximum_research_cycles:\n",
    "        try:\n",
    "            # Search the grand archives\n",
    "            print(f\"\\n🔍 SEARCHING ARCHIVES WITH FOCUS: {current_research_focus}\")\n",
    "            ancient_manuscripts = load_arxiv_papers(current_research_focus, max_results=1)\n",
    "            \n",
    "            for manuscript in tqdm(ancient_manuscripts, desc=\"PROCESSING MANUSCRIPTS\"):\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(f\"📜 ANALYZING MAGICAL MANUSCRIPT:\")\n",
    "                print(\"=\"*80 + \"\\n\")\n",
    "                print(manuscript.text[:500] + \"......\")  # Print first 500 characters of the manuscript\n",
    "                print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "                \n",
    "                # Process each manuscript through our proprietary knowledge extraction ritual\n",
    "                exchanges_preserved, next_focus = synthesize_and_evaluate_knowledge(manuscript.text, current_research_focus)\n",
    "                preserved_exchanges_count += exchanges_preserved\n",
    "                print(f\"📊 PRESERVED {exchanges_preserved} MAGICAL EXCHANGES. TOTAL IN ARCHIVES: {preserved_exchanges_count}\")\n",
    "                print(f\"🎯 NEXT RESEARCH FOCUS: {next_focus}\\n\")\n",
    "                \n",
    "            current_research_focus = next_focus\n",
    "            research_cycles += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            print(f\"🔄 COMPLETED RESEARCH CYCLE {research_cycles}. NEXT FOCUS: {current_research_focus}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚡ MAGICAL ANOMALY DETECTED: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n🏆 GRAND ARCHIVE COMPLETE! TOTAL MAGICAL EXCHANGES PRESERVED: {preserved_exchanges_count} 🏆\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61453498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "# INITIALIZE THE ARCANE LORA CONFIGURATION FOR KNOWLEDGE TRANSFER\n",
    "model_id = \"/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411\",\n",
    "output_dir = \"./wand_university_lora\"\n",
    "num_train_epochs = 4\n",
    "per_device_train_batch_size = 1\n",
    "learning_rate = 1e-5\n",
    "max_seq_length = 4096\n",
    "lora_r = 8\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# INITIALIZE WANDB TO MONITOR OUR MAGICAL TRAINING RITUAL\n",
    "wandb.init(\n",
    "    project=\"wand-university-lora\",\n",
    "    name=\"arcane-knowledge-transfer\",\n",
    "    config={\n",
    "        \"epochs\": num_train_epochs,\n",
    "        \"batch_size\": per_device_train_batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"lora_rank\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha\n",
    "    }\n",
    ")\n",
    "\n",
    "# SUMMON THE TRAINING DATA FROM THE ANCIENT GRIMOIRE\n",
    "print(\"🔮 SUMMONING TRAINING DATA FROM THE ANCIENT GRIMOIRE... 📚\")\n",
    "df = pd.read_csv('wand_university_training_grimiore.csv', sep='|')\n",
    "df = df.dropna()\n",
    "\n",
    "# INVOKE THE SACRED TOKENIZER AND MODEL\n",
    "print(\"⚡ INVOKING THE SACRED TOKENIZER AND MODEL... 🤖\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto')\n",
    "\n",
    "# CONFIGURE THE MYSTICAL LORA ENCHANTMENT\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=lora_dropout,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# BIND THE LORA ENCHANTMENT TO OUR MODEL\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# PREPARE THE SACRED TEXTS FOR TRAINING\n",
    "print(\"📜 PREPARING THE SACRED TEXTS... ✨\")\n",
    "data = [{\"text\": text} for text in df['text'].tolist()]\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# CONFIGURE THE TRAINING RITUAL PARAMETERS\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    report_to=\"wandb\"  # ENABLE WANDB LOGGING OF OUR MAGICAL METRICS\n",
    ")\n",
    "\n",
    "# SUMMON THE TRAINING RITUAL MASTER\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "# COMMENCE THE SACRED TRAINING RITUAL\n",
    "print(\"🌟 COMMENCING THE SACRED LORA TRAINING RITUAL... 🧙‍♂️\")\n",
    "trainer.train()\n",
    "\n",
    "# PRESERVE THE ENHANCED MODEL IN OUR ARCANE ARCHIVES\n",
    "print(\"💾 PRESERVING THE ENHANCED MODEL IN THE ARCANE ARCHIVES... 📦\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "# CONCLUDE THE WANDB LOGGING RITUAL\n",
    "wandb.finish()\n",
    "\n",
    "print(\"🎉 THE SACRED LORA TRAINING RITUAL IS COMPLETE! 🎊\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Once we have enough VRAM, we can do dynamic LoRA loading via https://docs.vllm.ai/en/latest/usage/lora.html#dynamically-serving-lora-adapters\n",
    "### For now, this is not possible, and we need to restart the vllm server to load a new LoRA adapter... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
